{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d9060a818c32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import pylab\n",
    "\n",
    "root_dir = os.path.abspath('../..')\n",
    "proj_dir = os.path.join(root_dir, 'project')\n",
    "data_dir= os.path.join(proj_dir, \"train\", \"Train_Aligned_Faces\")\n",
    "\n",
    "emotions=\"Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\"\n",
    "emotions=emotions.split(',')\n",
    "emotions=[e.replace(\" \",\"\") for e in emotions]\n",
    "\n",
    "temp1 = []\n",
    "temp2 = []\n",
    "for ind,emotion in enumerate(emotions):\n",
    "    temp=[]\n",
    "    for files in os.walk(os.path.join(data_dir, emotion)):\n",
    "        for filenames in files:\n",
    "            temp.append(filenames)\n",
    "    files=temp[2]\n",
    "    for img_name in files:\n",
    "        #print(img_name)\n",
    "        image_path = os.path.join(os.path.join(data_dir, emotion), img_name)\n",
    "        img = imread(image_path, flatten=True)\n",
    "        img = img.astype('float32')\n",
    "        img = np.reshape(img,(181*143,))\n",
    "        temp1.append(img)\n",
    "        temp2.append(ind)\n",
    "\n",
    "train_x = np.stack(temp1)\n",
    "train_y = np.stack(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 25883)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X_train.to_csv(\"X_train.csv\",index=False,index_label=False,header=False)\\nY_train.to_csv(\"Y_train.csv\",index=False,index_label=False,header=False)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=pd.DataFrame(train_x)\n",
    "Y_train=pd.DataFrame(train_y)\n",
    "\"\"\"X_train.to_csv(\"X_train.csv\",index=False,index_label=False,header=False)\n",
    "Y_train.to_csv(\"Y_train.csv\",index=False,index_label=False,header=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25873</th>\n",
       "      <th>25874</th>\n",
       "      <th>25875</th>\n",
       "      <th>25876</th>\n",
       "      <th>25877</th>\n",
       "      <th>25878</th>\n",
       "      <th>25879</th>\n",
       "      <th>25880</th>\n",
       "      <th>25881</th>\n",
       "      <th>25882</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34.050999</td>\n",
       "      <td>33.050999</td>\n",
       "      <td>34.050999</td>\n",
       "      <td>36.751999</td>\n",
       "      <td>38.268002</td>\n",
       "      <td>37.268002</td>\n",
       "      <td>28.587999</td>\n",
       "      <td>19.587999</td>\n",
       "      <td>21.886999</td>\n",
       "      <td>24.186001</td>\n",
       "      <td>...</td>\n",
       "      <td>13.912000</td>\n",
       "      <td>13.738000</td>\n",
       "      <td>14.738000</td>\n",
       "      <td>14.396000</td>\n",
       "      <td>15.869000</td>\n",
       "      <td>16.478001</td>\n",
       "      <td>16.799000</td>\n",
       "      <td>22.870001</td>\n",
       "      <td>37.196999</td>\n",
       "      <td>54.495998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.171000</td>\n",
       "      <td>16.181999</td>\n",
       "      <td>16.181999</td>\n",
       "      <td>21.139000</td>\n",
       "      <td>24.025000</td>\n",
       "      <td>28.395000</td>\n",
       "      <td>31.395000</td>\n",
       "      <td>29.395000</td>\n",
       "      <td>29.395000</td>\n",
       "      <td>30.591000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.947000</td>\n",
       "      <td>10.784000</td>\n",
       "      <td>11.197000</td>\n",
       "      <td>7.349000</td>\n",
       "      <td>8.224000</td>\n",
       "      <td>7.534000</td>\n",
       "      <td>8.947000</td>\n",
       "      <td>13.648000</td>\n",
       "      <td>13.235000</td>\n",
       "      <td>10.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.265999</td>\n",
       "      <td>50.380001</td>\n",
       "      <td>52.152000</td>\n",
       "      <td>53.152000</td>\n",
       "      <td>52.152000</td>\n",
       "      <td>52.152000</td>\n",
       "      <td>52.109001</td>\n",
       "      <td>51.293999</td>\n",
       "      <td>51.180000</td>\n",
       "      <td>51.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>164.694000</td>\n",
       "      <td>162.992996</td>\n",
       "      <td>160.705002</td>\n",
       "      <td>161.705002</td>\n",
       "      <td>164.292007</td>\n",
       "      <td>165.406006</td>\n",
       "      <td>166.220993</td>\n",
       "      <td>166.335007</td>\n",
       "      <td>165.035995</td>\n",
       "      <td>165.035995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.485000</td>\n",
       "      <td>18.186001</td>\n",
       "      <td>20.186001</td>\n",
       "      <td>23.186001</td>\n",
       "      <td>25.556000</td>\n",
       "      <td>27.812000</td>\n",
       "      <td>30.296000</td>\n",
       "      <td>32.296001</td>\n",
       "      <td>33.997002</td>\n",
       "      <td>35.595001</td>\n",
       "      <td>...</td>\n",
       "      <td>58.598000</td>\n",
       "      <td>58.712002</td>\n",
       "      <td>57.712002</td>\n",
       "      <td>57.712002</td>\n",
       "      <td>57.412998</td>\n",
       "      <td>56.929001</td>\n",
       "      <td>56.630001</td>\n",
       "      <td>55.743999</td>\n",
       "      <td>55.743999</td>\n",
       "      <td>54.743999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>124.716003</td>\n",
       "      <td>116.014999</td>\n",
       "      <td>117.014999</td>\n",
       "      <td>121.014999</td>\n",
       "      <td>120.901001</td>\n",
       "      <td>120.787003</td>\n",
       "      <td>110.787003</td>\n",
       "      <td>107.672997</td>\n",
       "      <td>125.619003</td>\n",
       "      <td>149.091995</td>\n",
       "      <td>...</td>\n",
       "      <td>29.492001</td>\n",
       "      <td>30.090000</td>\n",
       "      <td>29.503000</td>\n",
       "      <td>29.503000</td>\n",
       "      <td>29.802000</td>\n",
       "      <td>29.987000</td>\n",
       "      <td>29.987000</td>\n",
       "      <td>29.987000</td>\n",
       "      <td>30.987000</td>\n",
       "      <td>31.987000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25883 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0           1           2           3           4           5      \\\n",
       "0   34.050999   33.050999   34.050999   36.751999   38.268002   37.268002   \n",
       "1   18.171000   16.181999   16.181999   21.139000   24.025000   28.395000   \n",
       "2   50.265999   50.380001   52.152000   53.152000   52.152000   52.152000   \n",
       "3   15.485000   18.186001   20.186001   23.186001   25.556000   27.812000   \n",
       "4  124.716003  116.014999  117.014999  121.014999  120.901001  120.787003   \n",
       "\n",
       "        6           7           8           9         ...           25873  \\\n",
       "0   28.587999   19.587999   21.886999   24.186001     ...       13.912000   \n",
       "1   31.395000   29.395000   29.395000   30.591000     ...       11.947000   \n",
       "2   52.109001   51.293999   51.180000   51.180000     ...      164.694000   \n",
       "3   30.296000   32.296001   33.997002   35.595001     ...       58.598000   \n",
       "4  110.787003  107.672997  125.619003  149.091995     ...       29.492001   \n",
       "\n",
       "        25874       25875       25876       25877       25878       25879  \\\n",
       "0   13.738000   14.738000   14.396000   15.869000   16.478001   16.799000   \n",
       "1   10.784000   11.197000    7.349000    8.224000    7.534000    8.947000   \n",
       "2  162.992996  160.705002  161.705002  164.292007  165.406006  166.220993   \n",
       "3   58.712002   57.712002   57.712002   57.412998   56.929001   56.630001   \n",
       "4   30.090000   29.503000   29.503000   29.802000   29.987000   29.987000   \n",
       "\n",
       "        25880       25881       25882  \n",
       "0   22.870001   37.196999   54.495998  \n",
       "1   13.648000   13.235000   10.730000  \n",
       "2  166.335007  165.035995  165.035995  \n",
       "3   55.743999   55.743999   54.743999  \n",
       "4   29.987000   30.987000   31.987000  \n",
       "\n",
       "[5 rows x 25883 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "shuffle(training_data)\n",
    "\n",
    "trainx[\"predict\"]=trainy\n",
    "\n",
    "trainx.head()\n",
    "\n",
    "train_try=trainx.iloc[np.random.permutation(len(train_x))]\n",
    "\n",
    "train_try.head()\n",
    "\n",
    "y_try=train_try.as_matrix(train_try.columns.values[-1:])\n",
    "\n",
    "y_try=np.reshape(y_try,(891,))\n",
    "\n",
    "x_try=train_try.as_matrix(train_try.columns.values[:-1])\n",
    "\n",
    "train_try.columns.values[-1:]\n",
    "X_train=pd.DataFrame(train_x)\n",
    "train_try[train_try.columns.values[:-1]].to_csv(\"shuffled_X.csv\",index=False,index_label=False,header=False)\n",
    "train_try[train_try.columns.values[-1:]].to_csv(\"shuffled_Y.csv\",index=False,index_label=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp_x=pd.DataFrame(train_x)\n",
    "temp_x.to_csv(\"Normalized_train_reduced.csv\",index=False,index_label=False,header=False)\n",
    "\n",
    "temp_x.shape\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((7, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "train_results = [vectorized_result(y) for y in train_y]\n",
    "\n",
    "training_data = zip(train_x, train_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-e4d75ccb584f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlmbda\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmonitor_training_accuracy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abhinav/Desktop/5th sem/mlt/project/Untitled Folder/network2.pyc\u001b[0m in \u001b[0;36mSGD\u001b[1;34m(self, training_data, epochs, mini_batch_size, eta, lmbda, evaluation_data, monitor_evaluation_cost, monitor_evaluation_accuracy, monitor_training_cost, monitor_training_accuracy)\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmini_batch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                 self.update_mini_batch(\n\u001b[1;32m--> 166\u001b[1;33m                     mini_batch, eta, lmbda, len(training_data))\n\u001b[0m\u001b[0;32m    167\u001b[0m             \u001b[1;31m#print \"Epoch %s training complete\" % j\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmonitor_training_cost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abhinav/Desktop/5th sem/mlt/project/Untitled Folder/network2.pyc\u001b[0m in \u001b[0;36mupdate_mini_batch\u001b[1;34m(self, mini_batch, eta, lmbda, n)\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m             \u001b[0mnabla_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnb\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mnabla_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnw\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mdnw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdnw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnabla_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_nabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abhinav/Desktop/5th sem/mlt/project/Untitled Folder/network2.pyc\u001b[0m in \u001b[0;36mbackprop\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m             \u001b[0mnabla_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[0mnabla_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnabla_b\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnabla_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import network2\n",
    "mini_batch_size=10\n",
    "size=[9180,1000,100,7]\n",
    "#k=[20000,250]\n",
    "import datetime\n",
    "net = network2.Network(size)\n",
    "t1=datetime.datetime.now()\n",
    "print(net.SGD(training_data,50,mini_batch_size, 0.05,lmbda =0,monitor_training_accuracy=True)[3])\n",
    "t2=datetime.datetime.now()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = imread(image_path, flatten=True)\n",
    "img = img.astype('float32')\n",
    "\n",
    "from scipy.misc import imresize\n",
    "img= imresize(img, 0.6)\n",
    "\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        FullyConnectedLayer(n_in=25883, n_out=5000),\n",
    "        FullyConnectedLayer(n_in=5000, n_out=1000),\n",
    "        FullyConnectedLayer(n_in=1000, n_out=100),\n",
    "        SoftmaxLayer(n_in=1000, n_out=7)], mini_batch_size)\n",
    "\n",
    "net.SGD(training_data, 50, mini_batch_size, eta, training_data, training_data,lmbda=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 181, 143), \n",
    "                      filter_shape=(10, 1, 6, 6), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 10, 88, 69), \n",
    "                      filter_shape=(20, 10, 5, 6), \n",
    "                      poolsize=(2, 2), \n",
    "                      activation_fn=ReLU),\n",
    "        FullyConnectedLayer(\n",
    "            n_in=20*42*32, n_out=2000, activation_fn=ReLU, p_dropout=0.5),\n",
    "        FullyConnectedLayer(\n",
    "            n_in=2000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
    "        SoftmaxLayer(n_in=1000, n_out=7, p_dropout=0.5)], \n",
    "        mini_batch_size)\n",
    "net.SGD(training_data, 100, mini_batch_size, eta, training_data, training_data,lmbda=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Network([\n",
    "        ConvPoolLayer(image_shape=(mini_batch_size, 1, 181, 143), \n",
    "                      filter_shape=(20, 1, 6, 6), \n",
    "                      poolsize=(2, 2)),\n",
    "        FullyConnectedLayer(n_in=20*88*69, n_out=100),\n",
    "        SoftmaxLayer(n_in=100, n_out=7)], mini_batch_size)\n",
    "net.SGD(training_data, 50, mini_batch_size, eta, training_data, training_data,lmbda=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_save=[T.cast(param,\"float32\").eval() for param in net.params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "param_restore=[theano.shared(param,borrow=True) for param in param_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "theano.sandbox.cuda.var.CudaNdarraySharedVariable"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_accuracy = np.mean(\n",
    "                        [validate_mb_accuracy(j) for j in xrange(num_validation_batches)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhinav/miniconda3/envs/py27/lib/python2.7/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  if n_features < ensure_min_features:\n"
     ]
    }
   ],
   "source": [
    "temp1 = []\n",
    "for ind,emotion in enumerate(emotions):\n",
    "    temp=[]\n",
    "    for files in os.walk(os.path.join(data_dir, emotion)):\n",
    "        for filenames in files:\n",
    "            temp.append(filenames)\n",
    "    files=temp[2]\n",
    "    for img_name in files:\n",
    "        #print(img_name)\n",
    "        image_path = os.path.join(os.path.join(data_dir, emotion), img_name)\n",
    "        img = imread(image_path, flatten=True)\n",
    "        img = img.astype('float32')\n",
    "        img= imresize(img, 0.6)\n",
    "        img = np.reshape(img,(img.shape[0]*img.shape[1],))\n",
    "        temp1.append(img)\n",
    "\n",
    "train_x = np.stack(temp1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "train_x = MinMaxScaler().fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import cPickle\n",
    "filename=\"/home/abhinav/MNIST/data/mnist.pkl.gz\"\n",
    "f = gzip.open(filename, 'rb')\n",
    "train_data, validation_data, test_data = cPickle.load(f)\n",
    "f.close()\n",
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(890,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape\n",
    "\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shared_x = theano.shared(\n",
    "            np.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = theano.shared(\n",
    "    np.asarray(train_y, dtype=theano.config.floatX), borrow=True)\n",
    "training_data=(shared_x, T.cast(shared_y, \"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for loading the shuffled data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 25883)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce 920M (CNMeM is disabled, cuDNN not available)\n",
      "/home/abhinav/miniconda3/envs/py27/lib/python2.7/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
      "to set the GPU flag to False.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "trainy=pd.read_csv(\"./shuffled_Y.csv\",header=None)\n",
    "train_y=trainy.as_matrix()\n",
    "train_y=np.reshape(train_y,(891,))\n",
    "\n",
    "trainx=pd.read_csv(\"./shuffled_X.csv\",header=None)\n",
    "train_x=trainx.as_matrix()\n",
    "\n",
    "import network3 \n",
    "from network3  import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "mini_batch_size = 10\n",
    "eta=0.05\n",
    "\n",
    "from network3 import ReLU\n",
    "\n",
    "shared_x = theano.shared(\n",
    "            np.asarray(train_x, dtype=theano.config.floatX), borrow=True)\n",
    "shared_y = theano.shared(\n",
    "    np.asarray(train_y, dtype=theano.config.floatX), borrow=True)\n",
    "training_data=(shared_x, T.cast(shared_y, \"int32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Epoch 0: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 89\n",
      "Epoch 1: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 178\n",
      "Epoch 2: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 267\n",
      "Epoch 3: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 356\n",
      "Epoch 4: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 445\n",
      "Epoch 5: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 534\n",
      "Epoch 6: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 623\n",
      "Epoch 7: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 712\n",
      "Epoch 8: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 801\n",
      "Epoch 9: validation accuracy 20.00%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.00%\n",
      "Training mini-batch number 890\n",
      "Epoch 10: validation accuracy 20.11%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.11%\n",
      "Training mini-batch number 979\n",
      "Epoch 11: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 1068\n",
      "Epoch 12: validation accuracy 20.22%\n",
      "Training mini-batch number 1157\n",
      "Epoch 13: validation accuracy 20.11%\n",
      "Training mini-batch number 1246\n",
      "Epoch 14: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 1335\n",
      "Epoch 15: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 1424\n",
      "Epoch 16: validation accuracy 20.11%\n",
      "Training mini-batch number 1513\n",
      "Epoch 17: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 1602\n",
      "Epoch 18: validation accuracy 20.22%\n",
      "Training mini-batch number 1691\n",
      "Epoch 19: validation accuracy 20.22%\n",
      "Training mini-batch number 1780\n",
      "Epoch 20: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 1869\n",
      "Epoch 21: validation accuracy 20.00%\n",
      "Training mini-batch number 1958\n",
      "Epoch 22: validation accuracy 20.22%\n",
      "Training mini-batch number 2047\n",
      "Epoch 23: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2136\n",
      "Epoch 24: validation accuracy 20.11%\n",
      "Training mini-batch number 2225\n",
      "Epoch 25: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2314\n",
      "Epoch 26: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2403\n",
      "Epoch 27: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2492\n",
      "Epoch 28: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2581\n",
      "Epoch 29: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2670\n",
      "Epoch 30: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2759\n",
      "Epoch 31: validation accuracy 20.34%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.34%\n",
      "Training mini-batch number 2848\n",
      "Epoch 32: validation accuracy 20.45%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.45%\n",
      "Training mini-batch number 2937\n",
      "Epoch 33: validation accuracy 20.45%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.45%\n",
      "Training mini-batch number 3026\n",
      "Epoch 34: validation accuracy 20.56%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.56%\n",
      "Training mini-batch number 3115\n",
      "Epoch 35: validation accuracy 20.67%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.67%\n",
      "Training mini-batch number 3204\n",
      "Epoch 36: validation accuracy 20.79%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.79%\n",
      "Training mini-batch number 3293\n",
      "Epoch 37: validation accuracy 20.67%\n",
      "Training mini-batch number 3382\n",
      "Epoch 38: validation accuracy 20.90%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.90%\n",
      "Training mini-batch number 3471\n",
      "Epoch 39: validation accuracy 20.90%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.90%\n",
      "Training mini-batch number 3560\n",
      "Epoch 40: validation accuracy 20.90%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 20.90%\n",
      "Training mini-batch number 3649\n",
      "Epoch 41: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 3738\n",
      "Epoch 42: validation accuracy 20.90%\n",
      "Training mini-batch number 3827\n",
      "Epoch 43: validation accuracy 20.90%\n",
      "Training mini-batch number 3916\n",
      "Epoch 44: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4005\n",
      "Epoch 45: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4094\n",
      "Epoch 46: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4183\n",
      "Epoch 47: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4272\n",
      "Epoch 48: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4361\n",
      "Epoch 49: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4450\n",
      "Epoch 50: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4539\n",
      "Epoch 51: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4628\n",
      "Epoch 52: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4717\n",
      "Epoch 53: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4806\n",
      "Epoch 54: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4895\n",
      "Epoch 55: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 4984\n",
      "Epoch 56: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 5073\n",
      "Epoch 57: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 5162\n",
      "Epoch 58: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 5251\n",
      "Epoch 59: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 5340\n",
      "Epoch 60: validation accuracy 21.01%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.01%\n",
      "Training mini-batch number 5429\n",
      "Epoch 61: validation accuracy 21.12%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.12%\n",
      "Training mini-batch number 5518\n",
      "Epoch 62: validation accuracy 21.12%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.12%\n",
      "Training mini-batch number 5607\n",
      "Epoch 63: validation accuracy 21.35%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.35%\n",
      "Training mini-batch number 5696\n",
      "Epoch 64: validation accuracy 21.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.57%\n",
      "Training mini-batch number 5785\n",
      "Epoch 65: validation accuracy 21.35%\n",
      "Training mini-batch number 5874\n",
      "Epoch 66: validation accuracy 21.35%\n",
      "Training mini-batch number 5963\n",
      "Epoch 67: validation accuracy 21.46%\n",
      "Training mini-batch number 6052\n",
      "Epoch 68: validation accuracy 21.46%\n",
      "Training mini-batch number 6141\n",
      "Epoch 69: validation accuracy 21.46%\n",
      "Training mini-batch number 6230\n",
      "Epoch 70: validation accuracy 21.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.57%\n",
      "Training mini-batch number 6319\n",
      "Epoch 71: validation accuracy 21.46%\n",
      "Training mini-batch number 6408\n",
      "Epoch 72: validation accuracy 21.57%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.57%\n",
      "Training mini-batch number 6497\n",
      "Epoch 73: validation accuracy 21.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.69%\n",
      "Training mini-batch number 6586\n",
      "Epoch 74: validation accuracy 21.69%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.69%\n",
      "Training mini-batch number 6675\n",
      "Epoch 75: validation accuracy 21.91%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 21.91%\n",
      "Training mini-batch number 6764\n",
      "Epoch 76: validation accuracy 21.69%\n",
      "Training mini-batch number 6853\n",
      "Epoch 77: validation accuracy 21.80%\n",
      "Training mini-batch number 6942\n",
      "Epoch 78: validation accuracy 21.80%\n",
      "Training mini-batch number 7031\n",
      "Epoch 79: validation accuracy 22.25%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 22.25%\n",
      "Training mini-batch number 7120\n",
      "Epoch 80: validation accuracy 22.02%\n",
      "Training mini-batch number 7209\n",
      "Epoch 81: validation accuracy 22.02%\n",
      "Training mini-batch number 7298\n",
      "Epoch 82: validation accuracy 22.13%\n",
      "Training mini-batch number 7387\n",
      "Epoch 83: validation accuracy 22.13%\n",
      "Training mini-batch number 7476\n",
      "Epoch 84: validation accuracy 22.36%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 22.36%\n",
      "Training mini-batch number 7565\n",
      "Epoch 85: validation accuracy 22.47%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 22.47%\n",
      "Training mini-batch number 7654\n",
      "Epoch 86: validation accuracy 22.02%\n",
      "Training mini-batch number 7743\n",
      "Epoch 87: validation accuracy 22.13%\n",
      "Training mini-batch number 7832\n",
      "Epoch 88: validation accuracy 22.36%\n",
      "Training mini-batch number 7921\n",
      "Epoch 89: validation accuracy 22.92%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 22.92%\n",
      "Training mini-batch number 8010\n",
      "Epoch 90: validation accuracy 22.25%\n",
      "Training mini-batch number 8099\n",
      "Epoch 91: validation accuracy 22.81%\n",
      "Training mini-batch number 8188\n",
      "Epoch 92: validation accuracy 22.13%\n",
      "Training mini-batch number 8277\n",
      "Epoch 93: validation accuracy 22.58%\n",
      "Training mini-batch number 8366\n",
      "Epoch 94: validation accuracy 22.36%\n",
      "Training mini-batch number 8455\n",
      "Epoch 95: validation accuracy 22.58%\n",
      "Training mini-batch number 8544\n",
      "Epoch 96: validation accuracy 22.47%\n",
      "Training mini-batch number 8633\n",
      "Epoch 97: validation accuracy 22.36%\n",
      "Training mini-batch number 8722\n",
      "Epoch 98: validation accuracy 22.58%\n",
      "Training mini-batch number 8811\n",
      "Epoch 99: validation accuracy 22.58%\n",
      "Finished training network.\n",
      "Best validation accuracy of 22.92% obtained at iteration 8009\n",
      "Corresponding test accuracy of 22.92%\n",
      "After changing to best weights:\n",
      "\n",
      "validation accuracy:2292\n"
     ]
    }
   ],
   "source": [
    "net = Network([\n",
    "        FullyConnectedLayer(n_in=25883, n_out=5000),\n",
    "        FullyConnectedLayer(n_in=5000, n_out=1000),\n",
    "        SoftmaxLayer(n_in=1000, n_out=7)], mini_batch_size)\n",
    "\n",
    "net.SGD(training_data, 100, mini_batch_size, eta, training_data, training_data,lmbda=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_save=[T.cast(param,\"float32\").eval() for param in net.params]\n",
    "\n",
    "def sigmoid(z):\n",
    "    sig=1/(1+np.exp(-z))\n",
    "    return sig\n",
    "\n",
    "layer_2_z=train_x.dot(param_save[0])+np.ones((891,1)).dot(np.reshape(param_save[1],(1,param_save[1].size)))\n",
    "layer_2_a=sigmoid(layer_2_z)\n",
    "layer_3_z=layer_2_a.dot(param_save[2])+np.ones((891,1)).dot(np.reshape(param_save[3],(1,param_save[3].size)))\n",
    "layer_3_a=sigmoid(layer_3_z)\n",
    "layer_4_z=layer_3_a.dot(param_save[4])\n",
    "transfered_x=layer_4_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.255892255892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(transfered_x, train_y).predict(transfered_x)\n",
    "accuracy=np.mean(y_pred==train_y)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_transfered=pd.DataFtransfered_xfered_xX_train.csvfered_x)\n",
    "X_transfered.to_csv(\"transfered_X.csv\",index=False,index_label=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37710437710437711"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(transfered_x,train_y)\n",
    "y_ada_pred=clf.predict(transfered_x)\n",
    "np.mean(y_ada_pred==train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.params)\n",
    "\n",
    "type(param_save)\n",
    "\n",
    "param_save=param_save[:-1]\n",
    "len(param_save)\n",
    "\n",
    "(train_x.dot(param_save[0])+np.ones((891,1)).dot(np.reshape(param_save[1],(1,5000)))).shape\n",
    "\n",
    "sigmoid(np.asarray(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
